{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #enable interactive features. It is called before every plot.\n"
     ]
    }
   ],
   "source": [
    "#Importing Required Libraries for our project\n",
    "import xml.etree.cElementTree as etree  #handle XML document for parsing\n",
    "import re   #A regular expression is a special sequence of characters that helps you match or find other strings,using a specialized syntax held in a pattern.\n",
    "from tqdm import tqdm # for showing the processing of the document with the help of progress bar\n",
    "import os   #for using operating system dependent functionality.\n",
    "import nltk # platform to work with human language data\n",
    "from bs4 import BeautifulSoup  #library for pulling data out of HTML and XML files\n",
    "from html2text import html2text #Python script that converts a page of HTML into clean and easy-to-read plain ASCII text.\n",
    "import numpy as np   #used for scientific computing with Python\n",
    "import matplotlib.pyplot as plt   #Matplotlib is a Python 2D plotting library\n",
    "#import seaborn as sns\n",
    "%matplotlib notebook  #enable interactive features. It is called  before every plot.\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating function to clean xml document\n",
    "def clean_html(html):\n",
    "    \"\"\"\n",
    "    Copied from NLTK package.\n",
    "    Remove HTML markup from the given string.\n",
    "\n",
    "    :param html: the HTML string to be cleaned\n",
    "    :type html: str\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "\n",
    "    # First we remove inline JavaScript/CSS:\n",
    "    cleaned = re.sub(r\"(?is)<(script|style).*?>.*?(</\\1>)\", \"\", html.strip())\n",
    "    # Then we remove html comments. This has to be done before removing regular\n",
    "    # tags since comments can contain '>' characters.\n",
    "    cleaned = re.sub(r\"(?s)<!--(.*?)-->[\\n]?\", \"\", cleaned)\n",
    "    # Next we can remove the remaining tags:\n",
    "    cleaned = re.sub(r\"(?s)<.*?>\", \" \", cleaned)\n",
    "    # Finally, we deal with whitespace\n",
    "    cleaned = re.sub(r\"&nbsp;\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"  \", \" \", cleaned)\n",
    "    return cleaned.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to load file \n",
    "\n",
    "def loadFiles(directory):\n",
    "    DEBUG =False\n",
    "    l = []\n",
    "    for file in os.listdir(directory):  # os.listdir() function (that belongs to the OS module) to search through a given path.\n",
    "        data = directory + file         # combining \n",
    "        if DEBUG : print (data)\n",
    "        l.append(data)\n",
    "    l=sorted(l)\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#specifying the location of data and storing in the variable\n",
    "\n",
    "cancer_file_loc = \"./Dataset/train/cancer/\"\n",
    "non_cancer_file_loc = \"./Dataset/train/noncancer/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading cancer and non_cancer data\n",
    "files_cancer = loadFiles(cancer_file_loc)\n",
    "files_noncancer = loadFiles(non_cancer_file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the cancer data\n",
    "\n",
    "cancer=[]\n",
    "for i in tqdm(files_cancer):\n",
    "    xmlDoc = open(i, 'r')   #opening file in our system\n",
    "    xmlDocData = xmlDoc.read()#reading file \n",
    "    html=xmlDocData        # assigning xmlDocData to html\n",
    "    cleanhtml = clean_html(html)  #calling clean_html() to clean document/dataset\n",
    "    text = html2text(cleanhtml) \n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    text2 = soup.get_text()    # this method return text part of an entire document or a tag\n",
    "    cancer.append([text2,'c']) # appending the cleaned text to the cancer data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the non_cancer data\n",
    "\n",
    "noncancer=[]\n",
    "for i in tqdm(files_noncancer):\n",
    "    xmlDoc = open(i, 'r')\n",
    "    xmlDocData = xmlDoc.read()\n",
    "    html=xmlDocData\n",
    "    cleanhtml = clean_html(html)\n",
    "    text = html2text(cleanhtml)\n",
    "    soup = BeautifulSoup(html,\"lxml\")\n",
    "    text2 = soup.get_text()\n",
    "    noncancer.append([text2,'nc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd   #pandas is an open source library easy-to-use data structures and  is used fordata analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating Dataframe for cancer and non_cancer with column name text and lable\n",
    "cnT  = pd.DataFrame(cancer,columns=['text','lable'])\n",
    "NcnT = pd.DataFrame(noncancer,columns=['text','lable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#concatening cancer and non_cancer data\n",
    "\n",
    "cm = [cnT,NcnT] \n",
    "df = pd.concat(cm,ignore_index=True) #Concatenate pandas objects without using index values along the concatenate axis \n",
    "df = df.sample(frac=1).reset_index(drop=True)#Do not try to insert index into dataframe columns.This will not resets the index to the default integer index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#printing first five column labelled text and lable\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing required library for cleaning textual data\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer  #\"A mapping of terms to feature indices\" \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize # this library is used to tokenise word as well as sentence\n",
    "import re\n",
    "import string  #This contain number of function to process standard Python strings\n",
    "from nltk.corpus import stopwords  # inorder to work stopwords we use this libarary\n",
    "from nltk.tokenize import RegexpTokenizer   #extracts tokens with the help of provided regex pattern to split the text or by repeatedly matching the regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labelling cancer:1 and non_cancer:0\n",
    "\n",
    "data  = df\n",
    "data  = np.array(data['text'])\n",
    "lable  = df['lable'].map({\"c\":1,\"nc\":0})  # labelling cancer as 1 and non_cancer as 0\n",
    "lable  = np.array(lable)  #creates an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanDatset(holdtext):\n",
    "    dataset=[]\n",
    "    for i in tqdm(holdtext):        \n",
    "        sent_tokenize_list = sent_tokenize(i)  #tokenising the sentences\n",
    "        tokenized_reports = [word_tokenize(report) for report in sent_tokenize_list] #tokenising the words\n",
    "        regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "        tokenized_reports_no_punctuation = []   #list which will store data with no punctuation marks\n",
    "        \n",
    "        for review in tokenized_reports:  # loop for removing punctuation marks in a document\n",
    "            new_review = []\n",
    "            for token in review: \n",
    "                new_token = regex.sub(u'', token)\n",
    "                if not new_token == u'':\n",
    "                    new_review.append(new_token)\n",
    "            tokenized_reports_no_punctuation.append(new_review)  #appending the no punctuation document in the list  \n",
    "        tokenized_reports_no_stopwords = []\n",
    "        for report in tokenized_reports_no_punctuation:# loop for removing the stopword\n",
    "            new_term_vector = []\n",
    "            for word in report:\n",
    "                if not word in stopwords.words('english'):# removing english stopwords\n",
    "                    new_term_vector.append(word) #appending document in the variable new_term_vector without stopwords\n",
    "            tokenized_reports_no_stopwords.append(new_term_vector)\n",
    "        v=[]\n",
    "        for i in tokenized_reports_no_stopwords:#Loop for removing digit from the document\n",
    "            v.append(\" \".join(i))\n",
    "        gt=\" \".join(v)\n",
    "        text2=''.join([i for i in gt if not i.isdigit()])\n",
    "        toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)#this approach will remove *u* before stripping all punctuation.\n",
    "        fnlTxt=''.join(toker.tokenize(text2))\n",
    "        dataset.append(\" \".join(fnlTxt.split()))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the dataset\n",
    "dataset = cleanDatset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1=pd.DataFrame(dataset,columns=[\"text\"])# creating the dataframe after completely cleaning the dataset as t1 with colname text\n",
    "t2=pd.DataFrame(lable,columns=[\"lable\"])#creating the dataframe t2 which is labelled as lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trData = pd.concat([t1,t2],axis=1)   #  concatening dataframe t1 and t2\n",
    "trData.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "holdtext = np.array(trData['text'])  # creating array of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extracting maximum 5000 features\n",
    "vect = CountVectorizer(max_features=5000)\n",
    "vect.fit(holdtext)\n",
    "simple_train_dtm = vect.transform(holdtext) # transforming the dataset\n",
    "std   =  simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tab = pd.DataFrame(simple_train_dtm.toarray(),columns=vect.get_feature_names())\n",
    "tab.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Reduction < Select Important Features >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dimensionality reduction for selecting important features among extracted features\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Creating function to apply PCA to reduce dimension\n",
    "\n",
    "def dimensionReduction(nof):\n",
    "    pca = PCA(n_components=nof) \n",
    "    fit = pca.fit(std)          #fitting PCA to our dataset\n",
    "    tx = pd.DataFrame(pca.components_,columns=tab.columns)\n",
    "    td  = np.array(tx)\n",
    "    tmp = td[-1]\n",
    "    newA = np.copy(tmp)\n",
    "    pos=[]\n",
    "    for i in range(nof):\n",
    "        mx = np.where(newA==np.max(newA))\n",
    "        pos.append(mx[0][0])\n",
    "        newA[mx[0][0]]=-999\n",
    "    newcolname = tx.columns[pos]\n",
    "    val = []\n",
    "    for i in newcolname:\n",
    "        val.append(i)\n",
    "    redu_features = tab[val]\n",
    "    #print(tab.shape)\n",
    "    #print(redu_features.shape)\n",
    "    return redu_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split to train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redu_features = dimensionReduction(500)\n",
    "redu_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array(redu_features)\n",
    "y = np.array(trData['lable'].astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm # sklearn is module for working with machine learning tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC() # svm classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train) # fitting the classifier as training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing libraries for checking accuracy_score, confusion matrix, precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 = clf.predict(X_test) #fitting the classifier to prediction on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,result1)# drawing confusion matrix to check the performance of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy of the SVM Classifier \n",
    "accuracy_score(y_test,result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#library for precision, recall and F-measure\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_score = clf.decision_function(X_test)  # Distance of the X_test to the separating hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F-Measure Score of SVM Classifier\n",
    "print(f1_score(y_test,result1,average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the average precision score of SVM\n",
    "average_precision = average_precision_score(y_test, y_score)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGBoost is an implementation of gradient boosted decision trees\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost = xgb.XGBClassifier() # boosted tree classifire\n",
    "model_xgboost.fit(X_train,y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the accuracy of classifier\n",
    "\n",
    "pred = model_xgboost.predict(X_test)\n",
    "print(accuracy_score(pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for measuring the performance of classifier\n",
    "confusion_matrix(y_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#F-Measure of Boosted Tree\n",
    "print(f1_score(y_test,pred,average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(y_test,pred)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision, recall, _ = precision_recall_curve(y_test,pred )\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysis of effect Reduce the dimension of feature and compare between SVM and Boosted Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total selected features is 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v= np.arange(10,3005,500)\n",
    "sample = sorted(v,reverse=True)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmAc  = []\n",
    "boostAcc = []\n",
    "for i in tqdm(sample):\n",
    "    redu = dimensionReduction(i)\n",
    "    X = np.array(redu)\n",
    "    y = np.array(trData['lable'].astype('int'))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "    result1 = clf.predict(X_test)\n",
    "    svmAc.append(accuracy_score(y_test,result1))\n",
    "    model_xgboost.fit(X_train,y_train) \n",
    "    pred = model_xgboost.predict(X_test)\n",
    "    boostAcc.append(accuracy_score(pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ploting the graph of accuracy change by reducing the dimension/feature\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='chandu9523', api_key='M3OD0OVnLJOjy71qbS1P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECIFYING THE LABEL OF THE GRAPH AND THE LAYOUT OF THE GRAPH AND THE TITLE OF THE GRAPH\n",
    "trace1 = go.Bar(\n",
    "    x = sample,\n",
    "    y = svmAc,\n",
    "    name='SVM_acc'\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x = sample,\n",
    "    y = boostAcc,\n",
    "    name='Boost_acc'\n",
    ")\n",
    "\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    barmode='group',\n",
    "    title='Accuracy_Changes_by_reduce_the_feature',\n",
    "    xaxis=dict(\n",
    "        title='No_of_feature',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Accuracy',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='Accuracy_Changes_by_reduce_the_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
